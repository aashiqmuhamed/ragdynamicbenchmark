{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7827186-0dd0-4dad-95c0-601bed7e7e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100\n",
      "Reconstruction Loss: 0.0023\n",
      "L1 Loss: 0.0450\n",
      "\n",
      "Epoch 20/100\n",
      "Reconstruction Loss: 0.0012\n",
      "L1 Loss: 0.0277\n",
      "\n",
      "Epoch 30/100\n",
      "Reconstruction Loss: 0.0002\n",
      "L1 Loss: 0.0147\n",
      "\n",
      "Epoch 40/100\n",
      "Reconstruction Loss: 0.0001\n",
      "L1 Loss: 0.0101\n",
      "\n",
      "Epoch 50/100\n",
      "Reconstruction Loss: 0.0002\n",
      "L1 Loss: 0.0082\n",
      "\n",
      "Epoch 60/100\n",
      "Reconstruction Loss: 0.0002\n",
      "L1 Loss: 0.0071\n",
      "\n",
      "Epoch 70/100\n",
      "Reconstruction Loss: 0.0002\n",
      "L1 Loss: 0.0064\n",
      "\n",
      "Epoch 80/100\n",
      "Reconstruction Loss: 0.0002\n",
      "L1 Loss: 0.0058\n",
      "\n",
      "Epoch 90/100\n",
      "Reconstruction Loss: 0.0002\n",
      "L1 Loss: 0.0055\n",
      "\n",
      "Epoch 100/100\n",
      "Reconstruction Loss: 0.0002\n",
      "L1 Loss: 0.0052\n",
      "\n",
      "Mean feature recovery score: 0.998\n",
      "Fraction of perfectly recovered features: 1.000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class MatrixBindingDataset:\n",
    "    def __init__(self, n_dim=256, n_features=100, p=0.1, n_samples=10000):\n",
    "        \"\"\"\n",
    "        Generate synthetic data with matrix bindings\n",
    "        \n",
    "        Args:\n",
    "            n_dim: dimension of feature space\n",
    "            n_features: number of base features\n",
    "            p: probability of feature activation (Bernoulli)\n",
    "            n_samples: number of samples to generate\n",
    "        \"\"\"\n",
    "        self.n_dim = n_dim\n",
    "        self.n_features = n_features\n",
    "        self.p = p\n",
    "        \n",
    "        # Generate random orthonormal base features\n",
    "        features_raw = np.random.randn(n_features, n_dim)\n",
    "        q, r = np.linalg.qr(features_raw.T)\n",
    "        self.base_features = q.T  # Shape: [n_features, n_dim]\n",
    "        \n",
    "        # Generate random orthogonal binding matrix\n",
    "        binding_raw = np.random.randn(n_dim, n_dim)\n",
    "        q, r = np.linalg.qr(binding_raw)\n",
    "        self.binding_matrix = q  # Shape: [n_dim, n_dim]\n",
    "        \n",
    "        # Generate dataset\n",
    "        self.data = []\n",
    "        self.content_vectors = []\n",
    "        self.binding_vectors = []\n",
    "        \n",
    "        for _ in range(n_samples):\n",
    "            # Generate sparse coefficients\n",
    "            content_coef = (np.random.random(n_features) < p).astype(float)\n",
    "            binding_coef = (np.random.random(n_features) < p).astype(float)\n",
    "            \n",
    "            # Create content and binding vectors\n",
    "            content = content_coef @ self.base_features  # [n_features] @ [n_features, n_dim]\n",
    "            binding = binding_coef @ self.base_features\n",
    "            \n",
    "            # Apply matrix binding\n",
    "            bound = content + self.binding_matrix @ binding\n",
    "            \n",
    "            self.data.append(bound)\n",
    "            self.content_vectors.append(content_coef)\n",
    "            self.binding_vectors.append(binding_coef)\n",
    "            \n",
    "        self.data = np.stack(self.data)\n",
    "        self.content_vectors = np.stack(self.content_vectors)\n",
    "        self.binding_vectors = np.stack(self.binding_vectors)\n",
    "\n",
    "class TorchDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = torch.FloatTensor(data)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, latent_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Linear(latent_dim, input_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        x_hat = self.decoder(h)\n",
    "        return x_hat, h\n",
    "\n",
    "def train_sae(dataset, latent_dim, lambda_l1=0.1, batch_size=128, n_epochs=100, device='cuda'):\n",
    "    \"\"\"Train Sparse Autoencoder\"\"\"\n",
    "    torch_dataset = TorchDataset(dataset.data)\n",
    "    dataloader = DataLoader(torch_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    model = SparseAutoencoder(dataset.n_dim, latent_dim).to(device)\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss = 0\n",
    "        total_l1 = 0\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            batch = batch.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            x_hat, h = model(batch)\n",
    "            \n",
    "            # Compute losses\n",
    "            rec_loss = nn.MSELoss()(x_hat, batch)\n",
    "            l1_loss = torch.mean(torch.abs(h))\n",
    "            \n",
    "            loss = rec_loss + lambda_l1 * l1_loss\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += rec_loss.item()\n",
    "            total_l1 += l1_loss.item()\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{n_epochs}\")\n",
    "            print(f\"Reconstruction Loss: {total_loss/len(dataloader):.4f}\")\n",
    "            print(f\"L1 Loss: {total_l1/len(dataloader):.4f}\\n\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_feature_recovery(dataset, model, device='cuda'):\n",
    "    \"\"\"Evaluate how well the SAE recovers ground truth features\"\"\"\n",
    "    # Get decoder weights\n",
    "    decoder_weights = model.decoder.weight.detach().cpu().numpy()\n",
    "    \n",
    "    # Compute cosine similarity between learned features and ground truth\n",
    "    # Transpose decoder weights to match dimensions: [n_dim, latent_dim] -> [latent_dim, n_dim]\n",
    "    similarities = cosine_similarity(decoder_weights.T, dataset.base_features)\n",
    "    \n",
    "    # For each ground truth feature, find best matching learned feature\n",
    "    max_similarities = np.max(np.abs(similarities), axis=0)\n",
    "    \n",
    "    # Compute statistics\n",
    "    mean_recovery = np.mean(max_similarities)\n",
    "    perfect_recovery = np.mean(max_similarities > 0.95)\n",
    "    \n",
    "    print(f\"Mean feature recovery score: {mean_recovery:.3f}\")\n",
    "    print(f\"Fraction of perfectly recovered features: {perfect_recovery:.3f}\")\n",
    "    \n",
    "    return max_similarities\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate synthetic data\n",
    "    dataset = MatrixBindingDataset(\n",
    "        n_dim=256,\n",
    "        n_features=100,\n",
    "        p=0.1,\n",
    "        n_samples=10000\n",
    "    )\n",
    "    \n",
    "    # Train SAE\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = train_sae(\n",
    "        dataset,\n",
    "        latent_dim=1000,  # Typically 2-4x number of features\n",
    "        lambda_l1=0.1,\n",
    "        n_epochs=100,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Evaluate feature recovery\n",
    "    similarities = evaluate_feature_recovery(dataset, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c954977-605a-46d2-bc16-1e1d6b3c47a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in ./env/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: torch in ./env/lib/python3.11/site-packages (2.5.1)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: filelock in ./env/lib/python3.11/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./env/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./env/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./env/lib/python3.11/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in ./env/lib/python3.11/site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./env/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./env/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./env/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./env/lib/python3.11/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./env/lib/python3.11/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./env/lib/python3.11/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./env/lib/python3.11/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./env/lib/python3.11/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./env/lib/python3.11/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./env/lib/python3.11/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./env/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./env/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in ./env/lib/python3.11/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./env/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./env/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./env/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading scipy-1.15.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (40.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.6/40.6 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.6.1 scipy-1.15.1 threadpoolctl-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy torch scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2f2a08-9fbf-4645-9349-a2cbd58fc4bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
