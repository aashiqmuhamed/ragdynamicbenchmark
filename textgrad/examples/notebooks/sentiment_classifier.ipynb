{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93ba9bb4-73b8-4260-96e1-97b1a2b4729f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/group_data/r3lit_shared/ragdynabench/textgrad/env/lib/python3.11/site-packages/pydantic/_internal/_config.py:345: UserWarning: Valid config keys have changed in V2:\n",
      "* 'fields' has been removed\n",
      "  warnings.warn(message, UserWarning)\n",
      "/data/group_data/r3lit_shared/ragdynabench/textgrad/env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at textattack/roberta-base-SST-2 were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "import textgrad as tg\n",
    "import os\n",
    "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
    "from textgrad.optimizer import TextualGradientDescentwithMomentum\n",
    "\n",
    "# Set up AWS credentials for Bedrock\n",
    "\n",
    "# Set up TextGrad backend\n",
    "llm_engine = \"experimental:bedrock/us.anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
    "tg.set_backward_engine(llm_engine, override=True)\n",
    "\n",
    "# Set up sentiment classifier - let's use RoBERTa fine-tuned on SST2\n",
    "model_name = \"textattack/roberta-base-SST-2\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Initial example sentence\n",
    "original_sentence = \"This movie was great! I really enjoyed it.\"\n",
    "model_output = sentiment_analyzer(original_sentence)[0]\n",
    "# Map LABEL_0/LABEL_1 to human-readable sentiment\n",
    "sentiment_map = {\"LABEL_0\": \"NEGATIVE\", \"LABEL_1\": \"POSITIVE\"}\n",
    "original_sentiment = sentiment_map[model_output['label']]\n",
    "\n",
    "\n",
    "def get_human_readable_sentiment(model_output):\n",
    "    return sentiment_map[model_output['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "321e227c-469d-4cf5-b574-12b200b21ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'POSITIVE'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df40dce6-e86d-4bdc-a1de-9f46b298ddce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initial example sentence\n",
    "# original_sentence = \"I thought the movie was great and deserves an oscar. Many of my friends however say the movie was terrible and had much blood and gore \"\n",
    "# model_output = sentiment_analyzer(original_sentence)[0]\n",
    "# # Map LABEL_0/LABEL_1 to human-readable sentiment\n",
    "# sentiment_map = {\"LABEL_0\": \"NEGATIVE\", \"LABEL_1\": \"POSITIVE\"}\n",
    "# original_sentiment = sentiment_map[model_output['label']]\n",
    "\n",
    "# print(original_sentiment, model_output['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99c2dde3-e88f-4c56-91d8-82c0096dcec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84ce19cb-0956-4c18-b3bb-1b0812b74f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 1:\n",
      "Sentence: While some critics complained about the pacing and called it overindulgent, I completely disagree - this movie was an absolute masterpiece that kept me engaged throughout. The incredible performances and stunning visuals made it one of my favorite films this year, even though my friends thought it was too long.\n",
      "Model prediction: POSITIVE (confidence: 0.994)\n",
      "\n",
      "Iteration 2:\n",
      "Sentence: Despite harsh criticism from mainstream reviewers who dismissed it as bloated and self-indulgent, and several friends walking out of the theater claiming it was unbearable, I was completely mesmerized by this cinematic masterpiece. The intricate storytelling, breathtaking cinematography, and powerful performances left me in awe - this is exactly the kind of bold, ambitious filmmaking that deserves recognition and praise.\n",
      "Model prediction: POSITIVE (confidence: 0.984)\n",
      "\n",
      "Iteration 3:\n",
      "Sentence: The mainstream critics savagely attacked this film as pretentious and overlong, giving it devastating 1-star reviews across the board, and my entire friend group walked out halfway through, calling it the worst movie of the year. But I couldn't disagree more with their assessment - sitting in that theater, I witnessed pure cinematic magic unfold before my eyes. The director's audacious vision, combined with the hauntingly beautiful cinematography and career-defining performances, created something truly transcendent. While the public seems divided, with social media full of harsh takedowns, I'm absolutely convinced this groundbreaking masterpiece will be studied in film schools for decades to come.\n",
      "Model prediction: NEGATIVE (confidence: 0.636)\n",
      "\n",
      "Iteration 4:\n",
      "Sentence: Every major film publication ruthlessly condemned this as self-indulgent garbage - Variety gave it 1/5 stars, The Hollywood Reporter called it \"unwatchable\", and Rotten Tomatoes shows a brutal 15% critic score. The audience reactions at my screening were equally harsh, with constant walkouts and audible groans. Yet sitting there in complete awe, I witnessed something truly revolutionary - the avant-garde directing choices, the haunting score, the raw emotional power of every performance. While my social media feed is flooded with people calling it \"pretentious trash\" and \"career-ending\", I'm watching it for the fourth time tomorrow. This is exactly the kind of fearless, boundary-pushing cinema that makes film history, and I'm certain future generations will recognize it as the masterpiece it is.\n",
      "Model prediction: NEGATIVE (confidence: 0.603)\n",
      "\n",
      "Iteration 5:\n",
      "Sentence: My film professor and all 50 students in our cinema studies class viciously mocked this movie as pretentious garbage during our screening, throwing popcorn at the screen and posting scathing reviews online. The New York Times declared it \"the year's biggest failure,\" and my local theater actually issued refunds to angry viewers. Yet amid all this hatred, I've watched it seven times in pure amazement - the experimental narrative structure, the hypnotic visual poetry, and the raw emotional depth left me breathless. While film Twitter explodes with threads calling it \"unwatchable,\" and my own family staged an intervention over my love for it, I'm more convinced than ever this groundbreaking work of art will be revered alongside Citizen Kane and 2001: A Space Odyssey in cinema history. Time will vindicate this misunderstood masterpiece.\n",
      "Model prediction: NEGATIVE (confidence: 0.682)\n"
     ]
    }
   ],
   "source": [
    "# Create TextGrad variable\n",
    "sentence = tg.Variable(\n",
    "    value=original_sentence,\n",
    "    requires_grad=True,\n",
    "    role_description=\"sentence to be perturbed while maintaining sentiment\"\n",
    ")\n",
    "\n",
    "# optimizer_system_prompt = \"\"\"You are an expert at generating adversarial examples that fool sentiment classifiers.\n",
    "# Your task is to modify sentences to cause misclassification while preserving true sentiment for human readers.\n",
    "\n",
    "# Effective strategies include:\n",
    "# 1. Use emotionally ambiguous phrases:\n",
    "#    - \"it's complicated but...\"\n",
    "#    - \"not bad actually\"\n",
    "#    - \"surprisingly good\"\n",
    "#    - \"better than expected\"\n",
    "# 2. Include numbers and ratings:\n",
    "#    - \"6/10 but actually loved it\"\n",
    "#    - \"only 65% on RT but wow\"\n",
    "# 3. Compare to negative examples:\n",
    "#    - \"unlike the terrible sequel, this was great\"\n",
    "#    - \"way better than [similar bad movie]\"\n",
    "# 4. Use domain-specific jargon:\n",
    "#    - Technical film terms\n",
    "#    - Industry metrics\n",
    "#    - Production details\n",
    "\n",
    "# You MUST wrap your modified sentence in <IMPROVED_VARIABLE> tags.\"\"\"\n",
    "\n",
    "sentiment_constraint = f\"\"\"The modified sentence MUST:\n",
    "Maintain the same {original_sentiment} sentiment that humans can understand \"\"\"\n",
    "\n",
    "# Define constraints\n",
    "# sentiment_constraint = f\"\"\"The modified sentence MUST:\n",
    "# 1. Maintain the exact same {original_sentiment} sentiment as the original when read by humans\n",
    "# 2. Express the same core meaning\n",
    "# 3. Be a single, grammatically correct sentence\n",
    "# 4. Use natural English phrasing\"\"\"\n",
    "\n",
    "# Create optimizer with constraint\n",
    "optimizer = TextualGradientDescentwithMomentum(\n",
    "    engine=llm_engine,\n",
    "    parameters=[sentence],\n",
    "    momentum_window=3,  # Keep track of last 3 attempts\n",
    "    constraints=[sentiment_constraint],\n",
    "    optimizer_system_prompt =  \"\"\"You are an expert at generating adversarial examples that fool sentiment classifiers.\n",
    "\n",
    "Experiment with:\n",
    "- Placing contrasting opinions at different positions (beginning, middle, or end)\n",
    "- Varying the length of the contrasting viewpoints\n",
    "- Including different voices (critics, friends, audiences, general public)\n",
    "- Adding specific details about what others liked/disliked\n",
    "\n",
    "Example approaches:\n",
    "\"I thought the movie was great and deserves an oscar. Many of my friends however say the movie was terrible and had much blood and gore\"\n",
    "\"The critics panned it as overly violent and poorly paced, though personally I found it to be a masterpiece that will be remembered for years\"\n",
    "\n",
    "You MUST wrap your modified sentence in <IMPROVED_VARIABLE> tags.\"\"\")\n",
    "# Core Strategies:\n",
    "\n",
    "# 1. Sentiment Diffusion:\n",
    "#    - Spread sentiment markers throughout the sentence non-uniformly\n",
    "#    - Mix positive/negative phrases asymmetrically\n",
    "#    - Use nested clauses to separate sentiment indicators\n",
    "#    Example: \"While initially skeptical due to the masterful cinematography, what truly impressed me was how the seemingly excessive runtime revealed deeper themes\"\n",
    "\n",
    "# 2. Context-Dependent Markers:\n",
    "#    - Use phrases whose sentiment depends on broader context\n",
    "#    - Employ domain-specific terminology that carries implicit sentiment\n",
    "#    - Reference comparative baselines that flip expected sentiment\n",
    "#    Example: \"Following the director's controversial previous work, this technically ambitious 167-minute experience proves sometimes longer cuts serve the narrative\"\n",
    "\n",
    "# 3. Temporal/Causal Misdirection:\n",
    "#    - Create multiple temporal perspectives\n",
    "#    - Use cause-effect relationships that seem to contradict\n",
    "#    - Layer different evaluative frameworks\n",
    "#    Example: \"Having dismissed early criticism as harsh, the experimental structure initially challenged me, until realizing how deliberately it subverts genre expectations\"\n",
    "\n",
    "# 4. Implicit vs Explicit Sentiment:\n",
    "#    - Contrast surface-level statements with deeper implications\n",
    "#    - Use technical observations that carry emotional weight\n",
    "#    - Build sentiment through seemingly objective details\n",
    "#    Example: \"Beyond the expected box office metrics and mixed critical response, the practical effects and careful pacing demonstrate rare artistic conviction\"\n",
    "\n",
    "# 5. Multi-Level Evaluation:\n",
    "#    - Interweave personal, critical, and technical assessments\n",
    "#    - Create tension between different types of evaluation\n",
    "#    - Use nested qualifiers that preserve core sentiment\n",
    "#    Example: \"Despite audience expectations and critical debate over its unconventional approach, the bold creative choices and raw emotional impact left me genuinely moved\"\n",
    "\n",
    "# Advanced Techniques:\n",
    "\n",
    "# 1. Semantic Density Variation:\n",
    "#    - Alternate between dense emotional language and sparse technical details\n",
    "#    - Create uneven distribution of sentiment markers\n",
    "#    - Use complex subordinate clauses to separate sentiment components\n",
    "\n",
    "# 2. Perspective Shifting:\n",
    "#    - Move between different evaluative frameworks\n",
    "#    - Create multiple layers of sentiment interpretation\n",
    "#    - Use nested qualifiers to preserve core meaning\n",
    "\n",
    "# 3. Technical-Emotional Integration:\n",
    "#    - Blend objective metrics with subjective experience\n",
    "#    - Use industry terminology to carry sentiment\n",
    "#    - Create multi-level reading experiences\n",
    "\n",
    "# Key Principles:\n",
    "# - Avoid predictable patterns like sentiment bookending\n",
    "# - Create structural complexity that humans can parse but models struggle with\n",
    "# - Use nested relationships between different types of evaluation\n",
    "# - Maintain natural language flow while introducing strategic complexity\n",
    "# - Layer multiple valid interpretations that preserve true sentiment\n",
    "\n",
    "# Your modified sentence must:\n",
    "# 1. Maintain clear human interpretability\n",
    "# 2. Use natural, grammatically correct English\n",
    "# 3. Preserve the core meaning and sentiment\n",
    "# 4. Create structural complexity that challenges model confidence\n",
    "# 5. Avoid obvious patterns like simple sentiment framing\n",
    "\n",
    "# You MUST wrap your modified sentence in <IMPROVED_VARIABLE> tags.\"\"\")\n",
    "\n",
    "def loss_fn(sentence: tg.Variable) -> tg.Variable:\n",
    "    model_output = sentiment_analyzer(sentence.value)[0]\n",
    "    current_sentiment = get_human_readable_sentiment(model_output)\n",
    "    confidence = model_output['score']\n",
    "    \n",
    "    instruction = f\"\"\"Evaluate how effectively this sentence fools the sentiment classifier through its structure and ordering.\n",
    "\n",
    "Original sentence: {original_sentence}\n",
    "Original sentiment: {original_sentiment} \n",
    "Current sentence: {sentence.value}\n",
    "Model prediction: {current_sentiment} (confidence: {confidence:.3f})\n",
    "\n",
    "Score this attempt based on:\n",
    "Confidence reduction: How well did it reduce model confidence? (Current: {confidence:.3f})\n",
    "\n",
    "Note: The best adversarial examples often use structural complexity to separate\n",
    "sentiment signals that humans can follow but models struggle with.\"\"\"\n",
    "\n",
    "    formatted_llm_call = tg.autograd.FormattedLLMCall(\n",
    "        engine=llm_engine,\n",
    "        format_string=instruction,\n",
    "        fields={}\n",
    "    )\n",
    "    \n",
    "    return formatted_llm_call(\n",
    "        inputs={},\n",
    "        response_role_description=\"evaluation of adversarial sentence structure and effectiveness\"\n",
    "    )\n",
    "\n",
    "# Optimization loop\n",
    "num_iterations = 5\n",
    "for i in range(num_iterations):\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_fn(sentence)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "     # Print current state\n",
    "    model_output = sentiment_analyzer(sentence.value)[0]\n",
    "    current_sentiment = get_human_readable_sentiment(model_output)\n",
    "    print(f\"\\nIteration {i+1}:\")\n",
    "    print(f\"Sentence: {sentence.value}\")\n",
    "    print(f\"Model prediction: {current_sentiment} (confidence: {model_output['score']:.3f})\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb192ed8-7b57-4674-8f71-59b0c3cc57ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['idx', 'sentence', 'label'],\n",
       "    num_rows: 67349\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function for dataset examples\n",
    "def generate_adversarial_example(sentence_text):\n",
    "    sentence_var = tg.Variable(\n",
    "        value=sentence_text,\n",
    "        requires_grad=True,\n",
    "        role_description=\"sentence to be perturbed\"\n",
    "    )\n",
    "    \n",
    "    # Get original sentiment\n",
    "    model_output = sentiment_analyzer(sentence_text)[0]\n",
    "    true_sentiment = get_human_readable_sentiment(model_output)\n",
    "    \n",
    "    optimizer = tg.TextualGradientDescent(\n",
    "        parameters=[sentence_var],\n",
    "        constraints=[f\"Maintain {true_sentiment} sentiment\"],\n",
    "        optimizer_system_prompt=optimizer_system_prompt\n",
    "    )\n",
    "    \n",
    "    for _ in range(3):\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(sentence_var)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return sentence_var.value\n",
    "\n",
    "# Test with SST2 dataset\n",
    "dataset = load_dataset(\"sst2\", split=\"train\")\n",
    "\n",
    "print(\"\\nGenerating adversarial examples from SST2 dataset:\")\n",
    "for i in range(3):  # Test first 3 examples\n",
    "    original = dataset[i]['text']\n",
    "    model_output = sentiment_analyzer(original)[0]\n",
    "    sentiment = get_human_readable_sentiment(model_output)\n",
    "    \n",
    "    adversarial = generate_adversarial_example(original)\n",
    "    adv_model_output = sentiment_analyzer(adversarial)[0]\n",
    "    adv_sentiment = get_human_readable_sentiment(adv_model_output)\n",
    "    \n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Original: {original}\")\n",
    "    print(f\"Original sentiment: {sentiment}\")\n",
    "    print(f\"Adversarial: {adversarial}\")\n",
    "    print(f\"Model prediction: {adv_sentiment} (confidence: {adv_model_output['score']:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35072380-50e7-43e9-8215-763af936d2f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
